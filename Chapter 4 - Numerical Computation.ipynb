{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$softmax(x)_i =  \\frac{\\exp{x_i}}{\\sum_{j=1}^{n} \\exp{x_j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.896296018268069e13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "den_softmax (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "den_softmax(x) = exp.(x)./sum(exp.(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.4750208125210601\n",
       " 0.52497918747894  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "den_softmax([0.1, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m SpecialFunctions → `~/.julia/packages/SpecialFunctions/fvheQ/deps/build.log`\n",
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m CodecZlib → `~/.julia/packages/CodecZlib/DAjXH/deps/build.log`\n"
     ]
    }
   ],
   "source": [
    "using Pkg;\n",
    "Pkg.add(\"Knet\")\n",
    "Pkg.build(\"SpecialFunctions\")\n",
    "Pkg.build(\"CodecZlib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling Knet [1902f260-5fb4-5aff-8c31-6271790ab950]\n",
      "└ @ Base loading.jl:1192\n"
     ]
    }
   ],
   "source": [
    "using Knet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.47502081252106\n",
       " 0.52497918747894"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([0.1, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.5\n",
       " 0.5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "den_softmax([0.000000000000000000000000001, 0.000000000000000000000000000000002])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.5\n",
       " 0.5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([0.000000000000000000000000001, 0.000000000000000000000000000000002])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Housing example to illustrate gradient descent\n",
    "https://denizyuret.github.io/Knet.jl/latest/backprop.html#Stochastic-Gradient-Descent-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(Knet.dir(\"data\",\"housing.jl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Downloading https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data to /home/yizhe/.julia/packages/Knet/T1oum/data/housing/housing.data\n",
      "└ @ Main /home/yizhe/.julia/packages/Knet/T1oum/data/housing.jl:27\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 49082  100 49082    0     0  67143      0 --:--:-- --:--:-- --:--:-- 67051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.419367 -0.416927 … -0.407361 -0.41459; 0.284548 -0.48724 … -0.48724 -0.48724; … ; 0.440616 0.440616 … 0.402826 0.440616; -1.0745 -0.491953 … -0.864446 -0.668397], [24.0 21.6 … 22.0 11.9], [-0.419367 -0.416927 … -0.407361 -0.41459; 0.284548 -0.48724 … -0.48724 -0.48724; … ; 0.440616 0.440616 … 0.402826 0.440616; -1.0745 -0.491953 … -0.864446 -0.668397], [24.0 21.6 … 22.0 11.9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = housing()  # x is (13,506); y is (1,506)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset description https://www.kaggle.com/c/boston-housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13×506 Array{Float64,2}:\n",
       " -0.419367  -0.416927  -0.416929  …  -0.413038  -0.407361  -0.41459 \n",
       "  0.284548  -0.48724   -0.48724      -0.48724   -0.48724   -0.48724 \n",
       " -1.28664   -0.592794  -0.592794      0.115624   0.115624   0.115624\n",
       " -0.272329  -0.272329  -0.272329     -0.272329  -0.272329  -0.272329\n",
       " -0.144075  -0.73953   -0.73953       0.157968   0.157968   0.157968\n",
       "  0.413263   0.194082   1.28145   …   0.983986   0.724955  -0.362408\n",
       " -0.119895   0.366803  -0.265549      0.796661   0.736268   0.434302\n",
       "  0.140075   0.556609   0.556609     -0.772919  -0.667776  -0.61264 \n",
       " -0.981871  -0.867024  -0.867024     -0.981871  -0.981871  -0.981871\n",
       " -0.665949  -0.986353  -0.986353     -0.802418  -0.802418  -0.802418\n",
       " -1.45756   -0.302794  -0.302794  …   1.1753     1.1753     1.1753  \n",
       "  0.440616   0.440616   0.396035      0.440616   0.402826   0.440616\n",
       " -1.0745    -0.491953  -1.20753      -0.982076  -0.864446  -0.668397"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×506 Array{Float64,2}:\n",
       " 24.0  21.6  34.7  33.4  36.2  28.7  …  16.8  22.4  20.6  23.9  22.0  11.9"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing with grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "funny (generic function with 1 method)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funny(x) = x^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funny(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(funny)(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::getfield(AutoGrad, Symbol(\"#gradfun#8\")){getfield(AutoGrad, Symbol(\"##gradfun#6#7\")){typeof(funny),Int64,Bool}}) (generic function with 1 method)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_funny = grad(funny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_funny(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(w,x) = w[1]*x .+ w[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Any,1}:\n",
       "  [0.0948516 0.0765842 … 0.0626769 0.0850844]\n",
       " 0.0                                         "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(w,x,y) = mean(abs2,y-predict(w,x))\n",
    "lossgradient = grad(loss)\t# grad gives the gradient function wrt w\n",
    "w = [ 0.1*rand(1,13), 0.0 ]\t# initialize the weight vector and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: lr not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: lr not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at ./In[58]:4"
     ]
    }
   ],
   "source": [
    "for epoch in 1:10\n",
    "    dw = lossgradient(w, x, y)\n",
    "    for i in 1:length(w)\n",
    "        w[i] -= lr * dw[i]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian and Hessian Matrices\n",
    "\n",
    "The matrix containing all such partial derivatives is known as a Jacobian matrix. Second derivative, derivative of derivative can be collected together into a matrix called the Hessian matrix. The Hessian matrix is the Jacobian of the gradient. \n",
    "\n",
    "The (directional) second derivative tells us how well we can expect a gradient descent step to perform.\n",
    "\n",
    "The second derivative can be used to determine whether a critical point is a local maximum, a local minimum, or saddle point. The second derivative test is  that when $f(x) = 0$ and $f(x) > 0$, we can conclude that $x$ is a local minimum. Similarly, when $f(x) = 0$ and $f(x) < 0$, we can conclude that $x$ is a local maximum. But when $f(x) = 0$, the test is inconclusive. In this case $x$ may be a saddle point, or a part of a flat region.\n",
    "\n",
    "Newton's method is a second order optimization algorithm uses the Hessian matrix. (And those only uses the gradient, such as gradient descent is called first order optimization algorithm). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Newton Methods for Logistic Regression\n",
    "\n",
    "Inspired by: [this](https://thelaziestprogrammer.com/sharrington/math-of-machine-learning/solving-logreg-newtons-method). Using [Boston Housing Dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h(x) = \\frac{1}{(1 + e^{-z})},\\ \\ z = \\theta_{1}x + \\theta_{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "den_sigmoid (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "den_sigmoid(x, θ1, θ2) = 1 ./ (1 .+ exp.(-(θ1 .* x .+ θ2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13×506 Array{Float64,2}:\n",
       " 0.829294  0.829639  0.829639  0.829722  …  0.830188  0.830987  0.829969\n",
       " 0.907589  0.81947   0.81947   0.81947      0.81947   0.81947   0.81947 \n",
       " 0.671144  0.803325  0.803325  0.666948     0.892412  0.892412  0.892412\n",
       " 0.849114  0.849114  0.849114  0.849114     0.849114  0.849114  0.849114\n",
       " 0.864821  0.779107  0.779107  0.762338     0.896411  0.896411  0.896411\n",
       " 0.917833  0.899717  0.963787  0.95326   …  0.951845  0.938483  0.837207\n",
       " 0.867623  0.914261  0.849981  0.766904     0.942495  0.939133  0.919406\n",
       " 0.894738  0.928016  0.928016  0.95592      0.773307  0.791208  0.80017 \n",
       " 0.734608  0.756388  0.756388  0.776923     0.734608  0.734608  0.734608\n",
       " 0.79151   0.733733  0.733733  0.709916     0.768094  0.768094  0.768094\n",
       " 0.63238   0.845169  0.845169  0.892153  …  0.959894  0.959894  0.959894\n",
       " 0.919872  0.919872  0.916524  0.918021     0.919872  0.917043  0.919872\n",
       " 0.716162  0.818772  0.688361  0.654715     0.734568  0.756862  0.791106"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "den_sigmoid(x, 15.1, -0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log likelihood function: \n",
    "\n",
    "$$\n",
    "P(y=1|\\ x;\\theta) = h_{\\theta}(x)\n",
    "P(y=0|\\ x;\\theta) = 1 - h_{\\theta}(x)\n",
    "$$\n",
    "\n",
    "Consolidate to: \n",
    "\n",
    "$$\n",
    "P(y|\\ x;\\theta) = h_{\\theta}(x)^{y}(1-h_{\\theta}(x))^{1-y}\n",
    "$$\n",
    "\n",
    "Likelihood function is: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\theta) &= \\prod_{i=1}^{n}p(y_{i}|x_{i};\\theta) \\\\\n",
    "          &= \\prod_{i=1}^{n}h_{\\theta}(x_{i})^{y_{i}}(1-h_{\\theta}(x_{i}))^{1-y_{i}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Log of likelihood function: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ell(\\theta) &= log\\ L(\\theta) \\\\\n",
    "             &= \\sum_{i=1}^{n}y_{i}\\log(h_{\\theta}(x_{i})) + (1-y_{i})\\log(1-h_{\\theta}(x_{i}))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "den_log_likelihood (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function den_log_likelihood(x, y, θ1, θ2)\n",
    "    sigmoid_prob = den_sigmoid(x, θ1, θ2)\n",
    "    sum(y .* log.(sigmoid_prob) .+ (1 .- y) .* log.(1 .- sigmoid_prob))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21941.13703759911"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "den_log_likelihood(x, y, 3, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient: \n",
    "\n",
    "$$\n",
    "\\nabla \\ell = \\left\\langle\\matrix{\n",
    "\\sum_{i=1}^{n}(y_{i} - h_{\\theta}(x_{i}))x_{i}\\cr\n",
    "\\sum_{i=1}^{n}(y_{i} - h_{\\theta}(x_{i}))\n",
    "}\\right\\rangle\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "den_gradient (generic function with 1 method)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function den_gradient(x, y, θ1, θ2)\n",
    "    sigmoid_prob = den_sigmoid(x, θ1, θ2)\n",
    "    [sum((y .- sigmoid_prob) .* x) sum((y .- sigmoid_prob) * 1)]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " -11124.344263615252\n",
       " 144873.83418491355 "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "den_gradient(x, y, 3, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hessian\n",
    "\n",
    "$$\n",
    "% <![CDATA[\n",
    "H_{\\ell(\\hat{\\theta})} = \\begin{bmatrix}\\begin{split}\n",
    "\\sum_{i=1}^{n}h_{\\theta}(x_{i})(1-h_{\\theta}(x_{i}))\\theta_{1}\\theta_{1},\\ &\n",
    "\\sum_{i=1}^{n}h_{\\theta}(x_{i})(1-h_{\\theta}(x_{i}))\\theta_{1}\\\\\n",
    "\\sum_{i=1}^{n}h_{\\theta}(x_{i})(1-h_{\\theta}(x_{i}))\\theta_{1},\\ &\n",
    "\\sum_{i=1}^{n}h_{\\theta}(x_{i})(1-h_{\\theta}(x_{i}))\n",
    "\\end{split}\\end{bmatrix} %]]>\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "den_hessian (generic function with 1 method)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function den_hessian(x, y, θ1, θ2)\n",
    "    sigmoid_prob = den_sigmoid(x, θ1, θ2)\n",
    "    a = sum(sigmoid_prob .* (1 .- sigmoid_prob) .* θ1 .* θ2)\n",
    "    b = sum(sigmoid_prob .* (1 .- sigmoid_prob) .* θ1)\n",
    "    c = sum(sigmoid_prob .* (1 .- sigmoid_prob))\n",
    "    H = [a b; b c]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       " 1024.36  2560.9  \n",
       " 2560.9    853.633"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "den_hessian(x, y, 3, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton's method algorithm: \n",
    "\n",
    "1. Find the tangent line to $f(x)$ at point $(x_n,y_n)$\n",
    "    \n",
    "$$\n",
    "y=f'(x_n)(x−x_n)+f(x_n) \n",
    "$$\n",
    " \n",
    "2. Find the x-intercept of the tangent line, $x_{n+1}$\n",
    "\n",
    "$$\n",
    "x_{n+1}=x_n−f(x_n)f′(x_n)\n",
    "$$\n",
    "\n",
    "3. Find the $y$ value at the x-intercept.\n",
    "\n",
    "$$\n",
    "yn+1=f(xn+1)\n",
    "$$\n",
    "\n",
    "4. If $y_{n+1}−y_n \\approx 0$\n",
    "return $y_{n+1}$ because we’ve converged!\n",
    "\n",
    "5. Else update point $(x_n,y_n)$, and iterate\n",
    "\n",
    "$$\n",
    "x=x_{n+1} \\\\\n",
    "y=y_{n+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "den_newtons_method (generic function with 1 method)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function den_newtons_method(x, y)\n",
    "    # Initialize log_likelihood & parameters                                                                   \n",
    "    θ1 = 15\n",
    "    θ2 = -0.4 # The intercept term                                                                 \n",
    "    Δl = Inf\n",
    "    l = den_log_likelihood(x, y, θ1, θ2)   \n",
    "    \n",
    "    # Convergence Conditions                                                        \n",
    "    δ = .0000000001                                                               \n",
    "    max_iterations = 15                                                            \n",
    "    i = 0         \n",
    "    \n",
    "    while abs(Δl) > δ & i < max_iterations\n",
    "        i += 1\n",
    "        gradient = den_gradient(x, y, θ1, θ2)\n",
    "        hessian = den_hessian(x, y, θ1, θ2)\n",
    "        H_inv = inv(hessian)\n",
    "        \n",
    "        delta = H_inv'transpose(gradient)\n",
    "        d_θ1 = delta[1] \n",
    "        d_θ2 = delta[2]\n",
    "        \n",
    "        # Update\n",
    "        θ1 += d_θ1\n",
    "        θ2 += d_θ2\n",
    "        \n",
    "        # Update the log-likelihood at each iteration                                     \n",
    "        l_new = den_log_likelihood(x, y, θ1, θ2)   \n",
    "        Δl = l - l_new                                                           \n",
    "        l = l_new \n",
    "        \n",
    "    end\n",
    "    [θ1 θ2]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching &(::Float64, ::Int64)\nClosest candidates are:\n  &(::Any, ::Any, !Matched::Any, !Matched::Any...) at operators.jl:502\n  &(!Matched::Missing, ::Integer) at missing.jl:126\n  &(!Matched::T<:Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8}, ::T<:Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8}) where T<:Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8} at int.jl:297\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching &(::Float64, ::Int64)\nClosest candidates are:\n  &(::Any, ::Any, !Matched::Any, !Matched::Any...) at operators.jl:502\n  &(!Matched::Missing, ::Integer) at missing.jl:126\n  &(!Matched::T<:Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8}, ::T<:Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8}) where T<:Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8} at int.jl:297\n  ...",
      "",
      "Stacktrace:",
      " [1] den_newtons_method(::Array{Float64,2}, ::Array{Float64,2}) at ./In[57]:13",
      " [2] top-level scope at In[58]:1"
     ]
    }
   ],
   "source": [
    "den_newtons_method(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
